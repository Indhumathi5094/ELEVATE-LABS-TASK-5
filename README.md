 ğŸŒ³ Task 5: Decision Trees and Random Forests

 ğŸ“Œ Objective  
The goal of this task is to understand and implement tree-based machine learning models for both classification and regression.  
We focus on training Decision Trees, visualizing them, analyzing overfitting, and improving performance using Random Forests.

 ğŸ›  Tools & Libraries  
- ğŸ Python  
- ğŸ“š Scikit-learn (sklearn)  
- ğŸ“Š Matplotlib
- ğŸ§¾ Pandas

 ğŸ§ª Steps Performed  

1. âœ… Loaded Dataset  
   - Used the Breast Cancer dataset from `sklearn.datasets` for classification tasks.

2. ğŸŒ² Trained a Decision Tree Classifier  
   - Built a basic model using `DecisionTreeClassifier`.  
   - Visualized the tree structure to understand decision splits.

3. âœ‚ï¸ Analyzed Overfitting
   - Compared training and testing accuracy.  
   - Controlled complexity using parameters like `max_depth`.

4. ğŸŒ¿ Trained a Random Forest Model 
   - Implemented `RandomForestClassifier` for better accuracy and generalization.  
   - Compared its performance with the single decision tree.

5. ğŸ§  Interpreted Feature Importances 
   - Identified the top important features that impact predictions.

6. ğŸ” Evaluated Using Cross-Validation  
   - Performed 5-fold cross-validation for more reliable accuracy results.
     
 ğŸ“ˆ Results  

- ğŸŒ² Decision Tree showed high training accuracy but slightly lower test accuracy â†’ **sign of overfitting**.  
- âœ‚ï¸ Pruned trees (limited depth) improved model generalization.  
- ğŸŒ¿ Random Forest gave better accuracy and stability by combining multiple trees.  
- ğŸ“Š Feature Importance*highlighted the key predictors in the dataset.

 ğŸ“ Key Learnings  

- âœ… Decision Trees are simple and easy to interpret.  
- âœ¨ Controlling tree depth helps prevent overfitting.  
- ğŸŒ¿ Random Forest is a powerful ensemble method that improves accuracy.  
- ğŸ“Œ Cross-validation gives more trustworthy evaluation metrics.
